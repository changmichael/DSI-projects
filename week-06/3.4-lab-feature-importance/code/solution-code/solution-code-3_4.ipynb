{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison Lab\n",
    "\n",
    "In this lab we will compare the performance of all the models we have learned about so far, using the car evaluation dataset.\n",
    "\n",
    "> Instructor notes: This lab is long and can be completed successfully if points 1 and 2 are executed well. Help the class work through points 1 and 2 and then give them the solution if they are struggling, so they can complete points 3 and onwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the data\n",
    "\n",
    "The [car evaluation dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/car/) is in the assets/datasets folder. By now you should be very familiar with this dataset.\n",
    "\n",
    "1. Load the data into a pandas dataframe\n",
    "- Encode the categorical features properly: define a map that preserves the scale (assigning smaller numbers to words indicating smaller quantities)\n",
    "- Separate features from target into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>acceptability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  buying  maint doors persons lug_boot safety acceptability\n",
       "0  vhigh  vhigh     2       2    small    low         unacc\n",
       "1  vhigh  vhigh     2       2    small    med         unacc\n",
       "2  vhigh  vhigh     2       2    small   high         unacc\n",
       "3  vhigh  vhigh     2       2      med    low         unacc\n",
       "4  vhigh  vhigh     2       2      med    med         unacc"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../assets/datasets/car.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vhigh' 'high' 'med' 'low']\n",
      "['vhigh' 'high' 'med' 'low']\n",
      "['small' 'med' 'big']\n",
      "['low' 'med' 'high']\n",
      "['unacc' 'acc' 'vgood' 'good']\n",
      "['2' '4' 'more']\n",
      "['2' '3' '4' '5more']\n"
     ]
    }
   ],
   "source": [
    "print df.buying.unique()\n",
    "print df.maint.unique()\n",
    "print df.lug_boot.unique()\n",
    "print df.safety.unique()\n",
    "print df.acceptability.unique()\n",
    "print df.persons.unique()\n",
    "print df.doors.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map1 = {'low':1,\n",
    "        'med':2,\n",
    "        'high':3,\n",
    "        'vhigh':4}\n",
    "map2 = {'small':1,\n",
    "        'med':2,\n",
    "        'big':3}\n",
    "map3 = {'unacc':1,\n",
    "        'acc':2,\n",
    "        'good':3,\n",
    "        'vgood':4}\n",
    "map4 = {'2': 2,\n",
    "        '4': 4,\n",
    "        'more': 5}\n",
    "map5 = {'2': 2,\n",
    "        '3': 3,\n",
    "        '4': 4,\n",
    "        '5more': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   buying  maint  doors  persons  lug_boot  safety\n",
       "0       4      4      2        2         1       1\n",
       "1       4      4      2        2         1       2\n",
       "2       4      4      2        2         1       3\n",
       "3       4      4      2        2         2       1\n",
       "4       4      4      2        2         2       2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [c for c in df.columns if c != 'acceptability']\n",
    "dfn = df.copy()\n",
    "\n",
    "dfn.buying= df.buying.map(map1)\n",
    "dfn.maint= df.maint.map(map1)\n",
    "dfn.lug_boot = df.lug_boot.map(map2)\n",
    "dfn.persons = df.persons.map(map4)\n",
    "dfn.doors = df.doors.map(map5)\n",
    "dfn.safety = df.safety.map(map1)\n",
    "dfn.acceptability = df.acceptability.map(map3)\n",
    "\n",
    "X = dfn[features]\n",
    "y = dfn['acceptability']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1728 entries, 0 to 1727\n",
      "Data columns (total 7 columns):\n",
      "buying           1728 non-null int64\n",
      "maint            1728 non-null int64\n",
      "doors            1728 non-null int64\n",
      "persons          1728 non-null int64\n",
      "lug_boot         1728 non-null int64\n",
      "safety           1728 non-null int64\n",
      "acceptability    1728 non-null int64\n",
      "dtypes: int64(7)\n",
      "memory usage: 94.6 KB\n"
     ]
    }
   ],
   "source": [
    "dfn.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Useful preparation\n",
    "\n",
    "Since we will compare several models, let's write a couple of helper functions.\n",
    "\n",
    "1. Separate X and y between a train and test set, using 30% test set, random state = 42\n",
    "    - make sure that the data is shuffled and stratified\n",
    "2. Define a function called `evaluate_model`, that trains the model on the train set, tests it on the test, calculates:\n",
    "    - accuracy score\n",
    "    - confusion matrix\n",
    "    - classification report\n",
    "3. Initialize a global dictionary to store the various models for later retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "def evaluate_model(model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    a = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cr = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print cm\n",
    "    print cr\n",
    "    \n",
    "    return a\n",
    "\n",
    "all_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.a KNN\n",
    "\n",
    "Let's start with `KNeighborsClassifier`.\n",
    "\n",
    "1. Initialize a KNN model\n",
    "- Evaluate it's performance with the function you previously defined\n",
    "- Find the optimal value of K using grid search\n",
    "    - Be careful on how you perform the cross validation in the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[354   9   0   0]\n",
      " [  8 107   0   0]\n",
      " [  0   9  11   1]\n",
      " [  0   2   0  18]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.98      0.98      0.98       363\n",
      "          2       0.84      0.93      0.88       115\n",
      "          3       1.00      0.52      0.69        21\n",
      "          4       0.95      0.90      0.92        20\n",
      "\n",
      "avg / total       0.95      0.94      0.94       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "a = evaluate_model(KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "params = {'n_neighbors': range(2,60)}\n",
    "\n",
    "gsknn = GridSearchCV(KNeighborsClassifier(),\n",
    "                     params, n_jobs=-1,\n",
    "                     cv=KFold(len(y), n_folds=3, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=sklearn.cross_validation.KFold(n=1728, n_folds=3, shuffle=True, random_state=None),\n",
       "       error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsknn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 7}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsknn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93981481481481477"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsknn.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[357   6   0   0]\n",
      " [  6 108   1   0]\n",
      " [  1   8  11   1]\n",
      " [  0   2   0  18]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.98      0.98      0.98       363\n",
      "          2       0.87      0.94      0.90       115\n",
      "          3       0.92      0.52      0.67        21\n",
      "          4       0.95      0.90      0.92        20\n",
      "\n",
      "avg / total       0.95      0.95      0.95       519\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95183044315992293"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(gsknn.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_models['knn'] = {'model': gsknn.best_estimator_,\n",
    "                     'score': a}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b Bagging + KNN\n",
    "\n",
    "Now that we have found the optimal K, let's wrap `KNeighborsClassifier` in a BaggingClassifier and see if the score improves.\n",
    "\n",
    "1. Wrap the KNN model in a Bagging Classifier\n",
    "- Evaluate performance\n",
    "- Do a grid search only on the bagging classifier params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "baggingknn = BaggingClassifier(KNeighborsClassifier())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[352  11   0   0]\n",
      " [  9 105   1   0]\n",
      " [  0   8  12   1]\n",
      " [  0   3   1  16]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.98      0.97      0.97       363\n",
      "          2       0.83      0.91      0.87       115\n",
      "          3       0.86      0.57      0.69        21\n",
      "          4       0.94      0.80      0.86        20\n",
      "\n",
      "avg / total       0.94      0.93      0.93       519\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.93448940269749514"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(baggingknn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bagging_params = {'n_estimators': [10, 20],\n",
    "                  'max_samples': [0.7, 1.0],\n",
    "                  'max_features': [0.7, 1.0],\n",
    "                  'bootstrap_features': [True, False]}\n",
    "\n",
    "\n",
    "gsbaggingknn = GridSearchCV(baggingknn,\n",
    "                            bagging_params, n_jobs=-1,\n",
    "                            cv=KFold(len(y), n_folds=3, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=sklearn.cross_validation.KFold(n=1728, n_folds=3, shuffle=True, random_state=None),\n",
       "       error_score='raise',\n",
       "       estimator=BaggingClassifier(base_estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=10, n_jobs=1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [10, 20], 'max_samples': [0.7, 1.0], 'bootstrap_features': [True, False], 'max_features': [0.7, 1.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsbaggingknn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap_features': False,\n",
       " 'max_features': 1.0,\n",
       " 'max_samples': 1.0,\n",
       " 'n_estimators': 20}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsbaggingknn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[357   6   0   0]\n",
      " [  6 108   1   0]\n",
      " [  0   7  12   2]\n",
      " [  0   4   0  16]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.98      0.98      0.98       363\n",
      "          2       0.86      0.94      0.90       115\n",
      "          3       0.92      0.57      0.71        21\n",
      "          4       0.89      0.80      0.84        20\n",
      "\n",
      "avg / total       0.95      0.95      0.95       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_models['gsbaggingknn'] = {'model': gsbaggingknn.best_estimator_,\n",
    "                              'score': evaluate_model(gsbaggingknn.best_estimator_)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression\n",
    "\n",
    "Let's see if logistic regression performs better\n",
    "\n",
    "1. Initialize LR and test on Train/Test set\n",
    "- Find optimal params with Grid Search\n",
    "- See if Bagging improves the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[347  11   4   1]\n",
      " [ 59  53   3   0]\n",
      " [  5  15   1   0]\n",
      " [  0  19   0   1]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.84      0.96      0.90       363\n",
      "          2       0.54      0.46      0.50       115\n",
      "          3       0.12      0.05      0.07        21\n",
      "          4       0.50      0.05      0.09        20\n",
      "\n",
      "avg / total       0.73      0.77      0.74       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "all_models['lr'] = {'model': lr,\n",
    "                    'score': evaluate_model(lr)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l1', 'C': 10.0}\n",
      "0.830439814815\n",
      "[[344  14   4   1]\n",
      " [ 48  64   3   0]\n",
      " [  4  14   2   1]\n",
      " [  0   9   0  11]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.87      0.95      0.91       363\n",
      "          2       0.63      0.56      0.59       115\n",
      "          3       0.22      0.10      0.13        21\n",
      "          4       0.85      0.55      0.67        20\n",
      "\n",
      "avg / total       0.79      0.81      0.80       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "          'penalty': ['l1', 'l2']}\n",
    "\n",
    "gslr = GridSearchCV(lr,\n",
    "                    params, n_jobs=-1,\n",
    "                    cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "\n",
    "gslr.fit(X, y)\n",
    "\n",
    "print gslr.best_params_\n",
    "print gslr.best_score_\n",
    "\n",
    "all_models['gslr'] = {'model': gslr.best_estimator_,\n",
    "                             'score': evaluate_model(gslr.best_estimator_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 10, 'bootstrap_features': False}\n",
      "0.833912037037\n",
      "[[344  14   4   1]\n",
      " [ 49  63   3   0]\n",
      " [  3  16   1   1]\n",
      " [  0   9   0  11]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.87      0.95      0.91       363\n",
      "          2       0.62      0.55      0.58       115\n",
      "          3       0.12      0.05      0.07        21\n",
      "          4       0.85      0.55      0.67        20\n",
      "\n",
      "avg / total       0.78      0.81      0.79       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsbagginglr = GridSearchCV(BaggingClassifier(gslr.best_estimator_),\n",
    "                           bagging_params, n_jobs=-1,\n",
    "                           cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "\n",
    "gsbagginglr.fit(X, y)\n",
    "\n",
    "print gsbagginglr.best_params_\n",
    "print gsbagginglr.best_score_\n",
    "\n",
    "all_models['gsbagginglr'] = {'model': gsbagginglr.best_estimator_,\n",
    "                             'score': evaluate_model(gsbagginglr.best_estimator_)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decision Trees\n",
    "\n",
    "Let's see if Decision Trees perform better\n",
    "\n",
    "1. Initialize DT and test on Train/Test set\n",
    "- Find optimal params with Grid Search\n",
    "- See if Bagging improves the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "all_models['dt'] = {'model': dt,\n",
    "                    'score': evaluate_model(dt)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'splitter': ['best', 'random'],\n",
    "          'max_depth': [None, 5, 10],\n",
    "          'min_samples_split': [2, 5],\n",
    "          'min_samples_leaf': [1, 2, 3]}\n",
    "\n",
    "gsdt = GridSearchCV(dt,\n",
    "                    params, n_jobs=-1,\n",
    "                    cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "\n",
    "gsdt.fit(X, y)\n",
    "print gsdt.best_params_\n",
    "print gsdt.best_score_\n",
    "\n",
    "all_models['gsdt'] = {'model': gsdt.best_estimator_,\n",
    "                      'score': evaluate_model(gsdt.best_estimator_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsbaggingdt = GridSearchCV(BaggingClassifier(gsdt.best_estimator_),\n",
    "                           bagging_params, n_jobs=-1,\n",
    "                           cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "\n",
    "gsbaggingdt.fit(X, y)\n",
    "\n",
    "print gsbaggingdt.best_params_\n",
    "print gsbaggingdt.best_score_\n",
    "\n",
    "all_models['gsbaggingdt'] = {'model': gsbaggingdt.best_estimator_,\n",
    "                             'score': evaluate_model(gsbaggingdt.best_estimator_)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Support Vector Machines\n",
    "\n",
    "Let's see if SVM perform better\n",
    "\n",
    "1. Initialize SVM and test on Train/Test set\n",
    "- Find optimal params with Grid Search\n",
    "- See if Bagging improves the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "all_models['svm'] = {'model': svm,\n",
    "                     'score': evaluate_model(svm)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'C': [0.01, 0.1, 1.0, 10.0, 30.0, 100.0],\n",
    "          'gamma': ['auto', 0.1, 1.0, 10.0],\n",
    "          'kernel': ['linear', 'rbf']}\n",
    "\n",
    "\n",
    "gssvm = GridSearchCV(svm,\n",
    "                    params, n_jobs=-1,\n",
    "                    cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "\n",
    "gssvm.fit(X, y)\n",
    "print gssvm.best_params_\n",
    "print gssvm.best_score_\n",
    "\n",
    "all_models['gssvm'] = {'model': gssvm.best_estimator_,\n",
    "                      'score': evaluate_model(gssvm.best_estimator_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsbaggingsvm = GridSearchCV(BaggingClassifier(gssvm.best_estimator_),\n",
    "                           bagging_params, n_jobs=-1,\n",
    "                           cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "\n",
    "gsbaggingsvm.fit(X, y)\n",
    "\n",
    "print gsbaggingsvm.best_params_\n",
    "print gsbaggingsvm.best_score_\n",
    "\n",
    "all_models['gsbaggingsvm'] = {'model': gsbaggingsvm.best_estimator_,\n",
    "                             'score': evaluate_model(gsbaggingsvm.best_estimator_)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Random Forest & Extra Trees\n",
    "\n",
    "Let's see if Random Forest and Extra Trees perform better\n",
    "\n",
    "1. Initialize RF and ET and test on Train/Test set\n",
    "- Find optimal params with Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "all_models['rf'] = {'model': rf,\n",
    "                    'score': evaluate_model(rf)}\n",
    "\n",
    "\n",
    "\n",
    "et = ExtraTreesClassifier()\n",
    "all_models['et'] = {'model': et,\n",
    "                    'score': evaluate_model(et)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'n_estimators':[3, 5, 10, 50],\n",
    "          'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': [None, 3, 5],\n",
    "          'min_samples_split': [2,5],\n",
    "          'class_weight':[None, 'balanced']}\n",
    "\n",
    "\n",
    "gsrf = GridSearchCV(RandomForestClassifier(n_jobs=-1),\n",
    "                    params, n_jobs=-1,\n",
    "                    cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "\n",
    "gsrf.fit(X, y)\n",
    "print gsrf.best_params_\n",
    "print gsrf.best_score_\n",
    "\n",
    "all_models['gsrf'] = {'model': gsrf.best_estimator_,\n",
    "                      'score': evaluate_model(gsrf.best_estimator_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gset = GridSearchCV(RandomForestClassifier(n_jobs=-1),\n",
    "                    params, n_jobs=-1,\n",
    "                    cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "\n",
    "gset.fit(X, y)\n",
    "print gset.best_params_\n",
    "print gset.best_score_\n",
    "\n",
    "all_models['gset'] = {'model': gset.best_estimator_,\n",
    "                      'score': evaluate_model(gset.best_estimator_)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model comparison\n",
    "\n",
    "Let's compare the scores of the various models.\n",
    "\n",
    "1. Do a bar chart of the scores of the best models. Who's the winner on the train/test split?\n",
    "- Re-test all the models using a 3 fold stratified shuffled cross validation\n",
    "- Do a bar chart with errorbars of the cross validation average scores. is the winner the same?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = pd.DataFrame([(k, v['score']) for k, v in all_models.iteritems()],\n",
    "             columns=['model', 'score']).set_index('model').sort_values('score', ascending=False)\n",
    "\n",
    "\n",
    "scores.plot(kind='bar')\n",
    "plt.ylim(0.6, 1.1)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "\n",
    "def retest(model):\n",
    "    scores = cross_val_score(model, X, y,\n",
    "                             cv=StratifiedKFold(y, shuffle=True),\n",
    "                             n_jobs=-1)\n",
    "    m = scores.mean()\n",
    "    s = scores.std()\n",
    "    \n",
    "    return m, s\n",
    "\n",
    "for k, v in all_models.iteritems():\n",
    "    cvres = retest(v['model'])\n",
    "    print k, \n",
    "    all_models[k]['cvres'] = cvres\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvscores = pd.DataFrame([(k, v['cvres'][0], v['cvres'][1] ) for k, v in all_models.iteritems()],\n",
    "                        columns=['model', 'score', 'error']).set_index('model').sort_values('score', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(range(len(cvscores)), cvscores.score,\n",
    "                yerr=cvscores.error,\n",
    "                tick_label=cvscores.index)\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "plt.xticks(rotation=70)\n",
    "plt.ylim(0.6, 1.1)\n",
    "\n",
    "# cvscores.to_csv('../../../5.2-lesson/assets/datasets/car_evaluation/model_comparison.csv')\n",
    "cvscores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "We have encoded the data using a map that preserves the scale.\n",
    "Would our results have changed if we had encoded the categorical data using `pd.get_dummies` or `OneHotEncoder`  to encode them as binary variables instead?\n",
    "\n",
    "1. Repeat the analysis for this scenario. Is it better?\n",
    "- Experiment with other models or other parameters, can you beat your classmates best score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "onehotpipe = make_pipeline(OneHotEncoder(),\n",
    "                           dt)\n",
    "\n",
    "\n",
    "all_models['onehotpipe'] = {'model': onehotpipe,\n",
    "                            'score': evaluate_model(onehotpipe)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
